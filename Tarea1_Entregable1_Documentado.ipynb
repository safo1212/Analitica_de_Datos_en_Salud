{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Anal\u00edtica de datos en salud**\n",
    "\n",
    "Presentado por:\n",
    "\n",
    "* 2400452 - Jennifer Benavides Castillo\n",
    "* 2400479 - Cristhian David Cruz Mill\u00e1n\n",
    "* 2400794 - Sergio Alejandro Fierro Ospitia\n",
    "* 2400478 - Edwin Andr\u00e9s Lasso Rosero"
   ],
   "metadata": {
    "id": "xIOttaDEzaeS"
   },
   "id": "xIOttaDEzaeS"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Entregable 1:\n",
    "Extraer y preparar un conjunto de datos cl\u00ednico anotado en formato BIO para entrenamiento de modelos de reconocimiento de entidades nombradas (NER) de c\u00e1ncer de pulm\u00f3n.\n",
    "\n",
    "##Sirve para:\n",
    "\n",
    "* Procesar archivos cl\u00ednicos con anotaciones manuales de entidades (por ejemplo, enfermedades, tratamientos, fechas, etc.).\n",
    "\n",
    "* Convertir dichos archivos a estructuras compatibles con bibliotecas de procesamiento de lenguaje natural como datasets de Hugging Face.\n",
    "\n",
    "* Validar la integridad de las anotaciones (estructura de columnas, formatos BIO correctos).\n",
    "\n",
    "* Crear objetos Dataset para entrenar o evaluar modelos NER con arquitectura Transformers.\n",
    "\n",
    "##Se puede utilizar en:\n",
    "\n",
    "* Entrenamiento de modelos NLP para extracci\u00f3n autom\u00e1tica de entidades cl\u00ednicas.\n",
    "\n",
    "* Estandarizaci\u00f3n de datasets cl\u00ednicos para investigaci\u00f3n m\u00e9dica.\n",
    "\n",
    "* Preparaci\u00f3n de datos para estudios de miner\u00eda de texto en salud y aplicaciones de inteligencia artificial en medicina.\n",
    "\n"
   ],
   "metadata": {
    "id": "O8taBA3Xksmx"
   },
   "id": "O8taBA3Xksmx"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed349a-ec06-4559-83dd-a20d0566fc4a",
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77ed349a-ec06-4559-83dd-a20d0566fc4a",
    "outputId": "d22d7d31-e77f-4efb-ad1d-2c7909194756",
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers\n",
    "!pip install seqeval\n",
    "! pip install -U datasets evaluate\n",
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff1331-c6de-41c1-a1b1-0452bf485f7d",
   "metadata": {
    "id": "68ff1331-c6de-41c1-a1b1-0452bf485f7d"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, Features, Sequence, Value, ClassLabel\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def leer_archivo_bio(archivo_bio):\n",
    "    \"\"\"Lee un archivo CSV estilo BIO con tres columnas: ID, Token, Etiqueta.\"\"\"\n",
    "    datos = defaultdict(list)\n",
    "    with open(archivo_bio, 'r', encoding='utf-8') as f:\n",
    "        lector = csv.reader(f)\n",
    "        next(lector)  # Saltar encabezado si existe\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        id_anterior = None\n",
    "\n",
    "        for num_linea, fila in enumerate(lector, start=2):\n",
    "            if len(fila) != 3:\n",
    "                raise ValueError(f\"Error en l\u00ednea {num_linea}: {fila}. Se esperaban 3 columnas (ID, palabra, etiqueta).\")\n",
    "\n",
    "            id_actual, palabra, etiqueta = fila\n",
    "            if id_anterior is None:\n",
    "                id_anterior = id_actual\n",
    "\n",
    "            if id_actual != id_anterior:\n",
    "                datos[\"tokens\"].append(tokens)\n",
    "                datos[\"ner_tags\"].append(labels)\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                id_anterior = id_actual\n",
    "\n",
    "            tokens.append(palabra)\n",
    "            labels.append(etiqueta)\n",
    "\n",
    "        # Guardar \u00faltima oraci\u00f3n\n",
    "        if tokens:\n",
    "            datos[\"tokens\"].append(tokens)\n",
    "            datos[\"ner_tags\"].append(labels)\n",
    "\n",
    "    return datos\n",
    "\n",
    "def cargar_datasets_bio(rutas_archivos):\n",
    "    \"\"\"Carga archivos .bio y devuelve un DatasetDict.\"\"\"\n",
    "    datasets = {}\n",
    "    for nombre, ruta in rutas_archivos.items():\n",
    "        datos = leer_archivo_bio(ruta)\n",
    "        datasets[nombre] = Dataset.from_dict(datos)\n",
    "\n",
    "    return DatasetDict(datasets)"
   ],
   "metadata": {
    "id": "JfNsjeFl7TRP"
   },
   "id": "JfNsjeFl7TRP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# PASO 1: Despues de cargar los datos, primero se detecta todas las etiquetas \u00fanicas\n",
    "def detectar_etiquetas_unicas(rutas_archivos):\n",
    "    \"\"\"Detecta autom\u00e1ticamente todas las etiquetas \u00fanicas en archivos CSV con 3 columnas.\"\"\"\n",
    "    todas_etiquetas = set()\n",
    "\n",
    "    for ruta in rutas_archivos.values():\n",
    "        with open(ruta, 'r', encoding='utf-8') as f:\n",
    "            lector = csv.reader(f)\n",
    "            next(lector)  #Saltar encabezado\n",
    "            for fila in lector:\n",
    "                if len(fila) == 3:\n",
    "                    _, _, etiqueta = fila\n",
    "                    todas_etiquetas.add(etiqueta)\n",
    "\n",
    "    # Ordenamos las etiquetas para que 'O' sea la \u00faltima\n",
    "    etiquetas_ordenadas = sorted(todas_etiquetas - {'O'}) + ['O']\n",
    "    return etiquetas_ordenadas\n"
   ],
   "metadata": {
    "id": "HlYzHblx7hi1"
   },
   "id": "HlYzHblx7hi1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def corregir_formato_bio(ruta_archivo):\n",
    "    filas_corregidas = []\n",
    "\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "        lector = csv.reader(f)\n",
    "        encabezado = next(lector)\n",
    "        for fila in lector:\n",
    "            if len(fila) == 3:\n",
    "                fila[2] = fila[2].replace(\"B_\", \"B-\").replace(\"I_\", \"I-\")  # Corrige solo la etiqueta\n",
    "            filas_corregidas.append(fila)\n",
    "\n",
    "    # Escribir de nuevo el archivo con las etiquetas corregidas\n",
    "    with open(ruta_archivo, 'w', newline='', encoding='utf-8') as f:\n",
    "        escritor = csv.writer(f)\n",
    "        escritor.writerow(encabezado)  # Restaurar encabezado\n",
    "        escritor.writerows(filas_corregidas)\n",
    "\n",
    "# Corregir los tres archivos\n",
    "corregir_formato_bio(\"/content/sentences_train.csv\")\n",
    "corregir_formato_bio(\"/content/sentences_test.csv\")\n",
    "corregir_formato_bio(\"/content/sentences_dev.csv\")\n",
    "\n",
    "print(\"Etiquetas corregidas exitosamente en los tres archivos.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmVVpJGsRTWk",
    "outputId": "98ad96f6-3b65-4c80-f324-87a524f4c0f0"
   },
   "id": "vmVVpJGsRTWk",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Etiquetas corregidas exitosamente en los tres archivos.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Se definen los nombres de las rutas (paths) de los archivos .bio\n",
    "rutas_archivos = {\n",
    "    \"train\": \"/content/sentences_train.csv\",\n",
    "    \"test\": \"/content/sentences_test.csv\",\n",
    "    \"valid\": \"/content/sentences_dev.csv\"\n",
    "}\n"
   ],
   "metadata": {
    "id": "cQtmO3mPILMu"
   },
   "id": "cQtmO3mPILMu",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Detectar autom\u00e1ticamente todas las etiquetas\n",
    "LABELS = detectar_etiquetas_unicas(rutas_archivos)\n",
    "print(\"Etiquetas detectadas:\", LABELS)\n",
    "\n",
    "# Cargar los datasets\n",
    "dataset_dict = cargar_datasets_bio(rutas_archivos)\n",
    "\n",
    "\n",
    "# Definir la estructura de features con las etiquetas detectadas\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=LABELS))\n",
    "})\n",
    "\n",
    "# Aplicar el casting a cada split\n",
    "for split in dataset_dict:\n",
    "    dataset_dict[split] = dataset_dict[split].cast(features)\n",
    "\n",
    "# Mostrar informaci\u00f3n del dataset\n",
    "print(\"\\nDataset cargado correctamente:\")\n",
    "print(dataset_dict)\n",
    "\n",
    "# Mostrar un ejemplo del conjunto de entrenamiento\n",
    "print(\"\\nEjemplo del train:\")\n",
    "print(dataset_dict[\"train\"][0])\n",
    "\n",
    "# Mostrar las caracter\u00edsticas del dataset\n",
    "print(\"\\nCaracter\u00edsticas del dataset:\")\n",
    "print(dataset_dict[\"train\"].features)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532,
     "referenced_widgets": [
      "7d0700371b8747a990af6e246b07ee75",
      "83847d46f3bd491799ff4c1ea2f4df1c",
      "0a87ebcd376640cfb1e5c9d37b66901f",
      "9cd45948d91e44269f3361be4fdfc932",
      "4e9ebc3d54e64e5295aed003ebfc86ee",
      "66a83f77f5a6468fa91ba150caf8114d",
      "e291776f7a044458a2925a66e7344b2f",
      "f4c373b78ce14fe09c80d171b34ff9f2",
      "938c88be4b584d11adecbe276d8644e5",
      "e573026b199d4feaa8773cc43711e960",
      "868465d7618947a6bb798af8e498862c",
      "a51bbca4b43846a18f3b3ccb776a0dc1",
      "b60323dc953846f6ba0c4a698a074b32",
      "656afd9711ad4e42a38ce7fb14297be6",
      "328292b3e488469c844dd52849945f2c",
      "48d2a84aa7e64417ae22e406cb9f3705",
      "179e385bedb944d092627cef22537ede",
      "bcccd01288cc4854ba20fb29397e7c77",
      "377756a6ec7d4e55b7c5f4af4313b82d",
      "00b2c507a715434286700cca5cf92fb9",
      "f34539a6063c4e2f8456b9b2fe8bc618",
      "16ee992e111644d38a3945986813dcb1",
      "e60ef62efd9c4c5986ceb69e83e36442",
      "34b830287b6b4151bf9f32f41d64db40",
      "d0b166b3aa6e4be1b45b8b11e37cf165",
      "0a9bfb16db7640699acd61c70c60cd3f",
      "ea3285febbed46c09952dbfcdc42fcb5",
      "19b92efe43f44aa9b99440272d91a4ff",
      "b188e51c8c7e43399c8f3215c9eae416",
      "86a18377b084492f81ed8deb273d12ed",
      "da4c40e5df084b36857e9bfd55345fae",
      "5048c5a0b5e143cf97331c8d3f30ba80",
      "0d105195cc794d00b19b0f99d289ba8d"
     ]
    },
    "id": "s6zmsLRLEcg0",
    "outputId": "cbe079b7-53de-4244-dd33-8b1ffc297869"
   },
   "id": "s6zmsLRLEcg0",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Etiquetas detectadas: ['B-CANCER_CONCEPT', 'B-CHEMOTHERAPY', 'B-DATE', 'B-DRUG', 'B-FAMILY', 'B-FREQ', 'B-IMPLICIT_DATE', 'B-INTERVAL', 'B-METRIC', 'B-OCURRENCE_EVENT', 'B-QUANTITY', 'B-RADIOTHERAPY', 'B-SMOKER_STATUS', 'B-STAGE', 'B-SURGERY', 'B-TNM', 'I-CANCER_CONCEPT', 'I-DATE', 'I-DRUG', 'I-FAMILY', 'I-FREQ', 'I-IMPLICIT_DATE', 'I-INTERVAL', 'I-METRIC', 'I-OCURRENCE_EVENT', 'I-SMOKER_STATUS', 'I-STAGE', 'I-SURGERY', 'I-TNM', 'O']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/19154 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d0700371b8747a990af6e246b07ee75"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a51bbca4b43846a18f3b3ccb776a0dc1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/5453 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e60ef62efd9c4c5986ceb69e83e36442"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Dataset cargado correctamente:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 19154\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 4947\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 5453\n",
      "    })\n",
      "})\n",
      "\n",
      "Ejemplo del train:\n",
      "{'tokens': ['Abuela'], 'ner_tags': [4]}\n",
      "\n",
      "Caracter\u00edsticas del dataset:\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['B-CANCER_CONCEPT', 'B-CHEMOTHERAPY', 'B-DATE', 'B-DRUG', 'B-FAMILY', 'B-FREQ', 'B-IMPLICIT_DATE', 'B-INTERVAL', 'B-METRIC', 'B-OCURRENCE_EVENT', 'B-QUANTITY', 'B-RADIOTHERAPY', 'B-SMOKER_STATUS', 'B-STAGE', 'B-SURGERY', 'B-TNM', 'I-CANCER_CONCEPT', 'I-DATE', 'I-DRUG', 'I-FAMILY', 'I-FREQ', 'I-IMPLICIT_DATE', 'I-INTERVAL', 'I-METRIC', 'I-OCURRENCE_EVENT', 'I-SMOKER_STATUS', 'I-STAGE', 'I-SURGERY', 'I-TNM', 'O'], id=None), length=-1, id=None)}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9019-f67f-416b-aa7f-53bc0e7675ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6fe9019-f67f-416b-aa7f-53bc0e7675ad",
    "outputId": "cffaf900-1ef0-4368-ef79-27ed86e06da5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 19154\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 4947\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 5453\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = dataset_dict[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "print(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaHwGZnapyx7",
    "outputId": "a35be056-7fd3-4ea3-9b84-5dc38b7be909"
   },
   "id": "TaHwGZnapyx7",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['B-CANCER_CONCEPT', 'B-CHEMOTHERAPY', 'B-DATE', 'B-DRUG', 'B-FAMILY', 'B-FREQ', 'B-IMPLICIT_DATE', 'B-INTERVAL', 'B-METRIC', 'B-OCURRENCE_EVENT', 'B-QUANTITY', 'B-RADIOTHERAPY', 'B-SMOKER_STATUS', 'B-STAGE', 'B-SURGERY', 'B-TNM', 'I-CANCER_CONCEPT', 'I-DATE', 'I-DRUG', 'I-FAMILY', 'I-FREQ', 'I-IMPLICIT_DATE', 'I-INTERVAL', 'I-METRIC', 'I-OCURRENCE_EVENT', 'I-SMOKER_STATUS', 'I-STAGE', 'I-SURGERY', 'I-TNM', 'O']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf36109-9033-45dd-a9e8-28c1cbb6d8d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "8b9e3db0cb174aa89dfe717b0c1799b4",
      "392fc6fe90ca4d6ba2209a564d883fca",
      "b3ce9f0a72c040a2ac4f59a6c3171a7a",
      "d320b04efe334adcb616023b8e0e06ab",
      "d7e9989c00334050ab0c66fc70db9355",
      "cc3a98a254f640cb94e32518366d7c20",
      "07bb26d3cdf242b483a6ca82ed27bde2",
      "fce7d5c057104cc8a08bff088a378432",
      "af2753179b904f1d9806366e36f47233",
      "7f45d62255094732be8ca44658095708",
      "33761326d57143eeb4c842f481fff22e",
      "c834da0f448847019393968dcf9c5a36",
      "90afee70f8a542e08cbe21f0b4de1f27",
      "7c93d6159f9d460db9705f0793686ada",
      "893959f77e434beebca6fb9f4ff8297b",
      "2cc1969ce3cf459d8035402c6a041756",
      "f40502ff227e4c37b2f87f8ae74fda5b",
      "56aac267618741a68bb22895648c0fd7",
      "b3f1a95a60aa422bbbce88bd8f69f184",
      "d0297631150f45e2b4e0a54a2bda441d",
      "01ee8021aaaa4e85a92c59d0b0d88305",
      "96b852e58a754cf888cdb32bdc8a7319",
      "9fdcb199053f4586828aa2214233b906",
      "17084970e64145bc846f82221bd630a0",
      "3ac8b561450b4f55ac09457043c62d33",
      "f8c3c798bebb44f9af525e9377070a6c",
      "a507b24318b541209a525b3f04231d2b",
      "143fb268fe2b4ff7b900e654a31a6be1",
      "23b929df71ea48758d42fbb8364c4734",
      "b1238969b2654f66acb5bebb9fb914c2",
      "cfe13be0dcea40fb819b78ad8ed141ee",
      "f052779e49714b6391e020d3094868ff",
      "a64c606b68ce4a908a8f68e468c19c59"
     ]
    },
    "id": "cbf36109-9033-45dd-a9e8-28c1cbb6d8d8",
    "outputId": "24821943-1565-4d8d-8499-25324f336f14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/19154 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b9e3db0cb174aa89dfe717b0c1799b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c834da0f448847019393968dcf9c5a36"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5453 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fdcb199053f4586828aa2214233b906"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867c6bf-1ef7-4c17-bb14-465a76d23c62",
   "metadata": {
    "id": "d867c6bf-1ef7-4c17-bb14-465a76d23c62"
   },
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1fdc6-981b-4a36-acb8-de0427b90170",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5de1fdc6-981b-4a36-acb8-de0427b90170",
    "outputId": "b2e31255-369b-4577-e54d-099b9fc62755"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-CANCER_CONCEPT',\n",
       " 'B-CHEMOTHERAPY',\n",
       " 'B-DATE',\n",
       " 'B-DRUG',\n",
       " 'B-FAMILY',\n",
       " 'B-FREQ',\n",
       " 'B-IMPLICIT_DATE',\n",
       " 'B-INTERVAL',\n",
       " 'B-METRIC',\n",
       " 'B-OCURRENCE_EVENT',\n",
       " 'B-QUANTITY',\n",
       " 'B-RADIOTHERAPY',\n",
       " 'B-SMOKER_STATUS',\n",
       " 'B-STAGE',\n",
       " 'B-SURGERY',\n",
       " 'B-TNM',\n",
       " 'I-CANCER_CONCEPT',\n",
       " 'I-DATE',\n",
       " 'I-DRUG',\n",
       " 'I-FAMILY',\n",
       " 'I-FREQ',\n",
       " 'I-IMPLICIT_DATE',\n",
       " 'I-INTERVAL',\n",
       " 'I-METRIC',\n",
       " 'I-OCURRENCE_EVENT',\n",
       " 'I-SMOKER_STATUS',\n",
       " 'I-STAGE',\n",
       " 'I-SURGERY',\n",
       " 'I-TNM',\n",
       " 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "label_list = dataset_dict[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e5fa0-b5bd-4623-9d1e-4035af809db1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc1e5fa0-b5bd-4623-9d1e-4035af809db1",
    "outputId": "ff91de9f-b878-48fb-c8be-8570bfd9bc85"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ab71a-e634-4b50-94b1-6b4aa063b809",
   "metadata": {
    "id": "a57ab71a-e634-4b50-94b1-6b4aa063b809"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6495a-fd53-483c-a5a1-cd7109a9f6f5",
   "metadata": {
    "id": "3df6495a-fd53-483c-a5a1-cd7109a9f6f5"
   },
   "outputs": [],
   "source": [
    "model_bert_base = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_bert_base}-finetuned-{task}-lung\",\n",
    "    eval_strategy = \"epoch\", # Changed from evaluation_strategy to eval_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    hub_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ce404-0aad-4b1d-b0e9-e83c426cfa71",
   "metadata": {
    "id": "cb3ce404-0aad-4b1d-b0e9-e83c426cfa71"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import load_metric  # Para versiones antiguas\n",
    "    metric = load_metric(\"seqeval\")\n",
    "except ImportError:\n",
    "    from evaluate import load  # Para versiones nuevas\n",
    "    metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6549fe-dd9c-401c-8c73-652f55167c57",
   "metadata": {
    "id": "1e6549fe-dd9c-401c-8c73-652f55167c57"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec6d78-2476-457b-931d-385a22c0b649",
   "metadata": {
    "id": "adec6d78-2476-457b-931d-385a22c0b649"
   },
   "source": [
    "Validaci\u00f3n (eval_dataset): Se usa durante el entrenamiento para:\n",
    "\n",
    "Ajustar hiperpar\u00e1metros\n",
    "\n",
    "Detener el entrenamiento temprano (early stopping)\n",
    "\n",
    "Monitorizar el progreso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb63c1e-77d2-4fb9-a0f5-953a16ffc345",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cb63c1e-77d2-4fb9-a0f5-953a16ffc345",
    "outputId": "83491c19-f850-4c4b-b38e-23a52c704f74"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-19-08bf5cda74a4>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadcb0d-8cff-475d-9c67-c6db1cc31c7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "9fadcb0d-8cff-475d-9c67-c6db1cc31c7f",
    "outputId": "c1f12bef-1528-4333-e993-d7e83a1eed77"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjennifer-benavides\u001b[0m (\u001b[33mjennifer-benavides-universidad-del-valle\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250606_220641-5z7nrrgz</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jennifer-benavides-universidad-del-valle/huggingface/runs/5z7nrrgz' target=\"_blank\">bert-base-uncased-finetuned-ner-lung</a></strong> to <a href='https://wandb.ai/jennifer-benavides-universidad-del-valle/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/jennifer-benavides-universidad-del-valle/huggingface' target=\"_blank\">https://wandb.ai/jennifer-benavides-universidad-del-valle/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/jennifer-benavides-universidad-del-valle/huggingface/runs/5z7nrrgz' target=\"_blank\">https://wandb.ai/jennifer-benavides-universidad-del-valle/huggingface/runs/5z7nrrgz</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23950' max='23950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23950/23950 1:03:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.098779</td>\n",
       "      <td>0.929193</td>\n",
       "      <td>0.929193</td>\n",
       "      <td>0.929193</td>\n",
       "      <td>0.974055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.092315</td>\n",
       "      <td>0.932691</td>\n",
       "      <td>0.955427</td>\n",
       "      <td>0.943922</td>\n",
       "      <td>0.979143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.095306</td>\n",
       "      <td>0.925725</td>\n",
       "      <td>0.951944</td>\n",
       "      <td>0.938652</td>\n",
       "      <td>0.976736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.098728</td>\n",
       "      <td>0.937393</td>\n",
       "      <td>0.957632</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.979555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.103652</td>\n",
       "      <td>0.936550</td>\n",
       "      <td>0.950900</td>\n",
       "      <td>0.943670</td>\n",
       "      <td>0.979234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.110561</td>\n",
       "      <td>0.934635</td>\n",
       "      <td>0.956007</td>\n",
       "      <td>0.945200</td>\n",
       "      <td>0.979578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.110713</td>\n",
       "      <td>0.934957</td>\n",
       "      <td>0.949391</td>\n",
       "      <td>0.942118</td>\n",
       "      <td>0.978639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.114519</td>\n",
       "      <td>0.933106</td>\n",
       "      <td>0.952060</td>\n",
       "      <td>0.942488</td>\n",
       "      <td>0.978524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.121346</td>\n",
       "      <td>0.934230</td>\n",
       "      <td>0.951364</td>\n",
       "      <td>0.942719</td>\n",
       "      <td>0.978432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.122552</td>\n",
       "      <td>0.933828</td>\n",
       "      <td>0.950087</td>\n",
       "      <td>0.941887</td>\n",
       "      <td>0.978570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23950, training_loss=0.06487054183736972, metrics={'train_runtime': 3802.927, 'train_samples_per_second': 50.366, 'train_steps_per_second': 6.298, 'total_flos': 5513628084726960.0, 'train_loss': 0.06487054183736972, 'epoch': 10.0})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda24f5-5329-4651-be98-92afb588708a",
   "metadata": {
    "id": "6cda24f5-5329-4651-be98-92afb588708a"
   },
   "source": [
    "Buenas pr\u00e1cticas:\n",
    "\n",
    "No uses test para tomar decisiones: Solo para la evaluaci\u00f3n final\n",
    "\n",
    "Usa validaci\u00f3n para ajustes: Early stopping, learning rate, etc.\n",
    "\n",
    "Guarda test para el final: Como si fuera datos \"reales\" que el modelo nunca ha visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af7bb5-af06-4337-b9cf-d8ee8f3bcde7",
   "metadata": {
    "id": "38af7bb5-af06-4337-b9cf-d8ee8f3bcde7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "outputId": "bff5ce6b-07c4-4b18-e3fd-8f5ac2477e56"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='619' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [619/619 00:16]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "==================================================\n",
      "Resultados finales en conjunto de test:\n",
      "F1-score: 0.934\n",
      "Precisi\u00f3n: 0.921\n",
      "Recall: 0.948\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Resultados finales en conjunto de test:\")\n",
    "print(f\"F1-score: {test_metrics['eval_f1']:.3f}\")\n",
    "print(f\"Precisi\u00f3n: {test_metrics['eval_precision']:.3f}\")\n",
    "print(f\"Recall: {test_metrics['eval_recall']:.3f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a946a4-f5e0-4a87-a764-f4b4479a8c8b",
   "metadata": {
    "id": "d3a946a4-f5e0-4a87-a764-f4b4479a8c8b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252,
     "referenced_widgets": [
      "a9c1317f7f0347cd94a7ed2d2bfd08aa",
      "1e083b7c250048bb9b22a27c96cc7047",
      "d3974d36321a4498a5a72ed54e41791b",
      "ab92dd313de1404cb2f739a50695d47e",
      "e1e3ee80e1fa49e5949bfbd42b5f0a68",
      "c2b5b40d0bec449ab48390ad33137192",
      "b8382108a23143d5acc1010038b06417",
      "abca9b446005455586e80fcefff558f0",
      "7bd2cad5a28c416eabc20affe8d05f9c",
      "3aeabafeff464950a67d7e7964e044f7",
      "dd14559957c84fb891e630863b95eb5c",
      "877f3fc2210a4d4cb403d0fc5cab32b5",
      "59de9838b61a439f9103661f24288482",
      "f777896d778a4904bd3bf9a51875c8ed",
      "0c9a4a4722764399ae0abccca5ba8b86",
      "ee81973cebb245dea508fe0a13907de2",
      "1f359f1119934b90a9d21a617286567b",
      "1e5cbcb7a6b14515becc87f8bf248538",
      "1eca34f5d4384f13bce266683e81a0a9",
      "fe4a7c8ffa934f49b7cbe7d6b21c9f86",
      "baf377da207742cf9ee2fac67c8b2195",
      "d9be68f954844903ba3e27f704a6b52c",
      "8dcfb2233ac04ffc88483adec3cc9a89",
      "97d4e492119242eb8ba6e819d95d21cd",
      "bdee827dafe840dba3c7f08fdd7b3828",
      "38a415be4d4647199c934aa117e7ee35",
      "8a29a9c1dfa04f26aa40d109a8a1eb5c",
      "6c551fc0a3154e82983a9ba203e7153a",
      "61c2757266dc4467a82e0c32a3b70cf0",
      "4c4b84f1b8eb4779a83e49bde1a78b2e",
      "26c7f3683c014ccb8e19a4995722bbd3",
      "b4179fcbe19b4b82bf67b0f0a417d0e7",
      "3ec3c645d8274037b3e023f8789e62b8"
     ]
    },
    "outputId": "302ce3e4-996f-46d4-d983-27402eef4533"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9c1317f7f0347cd94a7ed2d2bfd08aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "events.out.tfevents.1749247600.4c1b94190950.10931.0:   0%|          | 0.00/21.3k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "877f3fc2210a4d4cb403d0fc5cab32b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "events.out.tfevents.1749251429.4c1b94190950.10931.1:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8dcfb2233ac04ffc88483adec3cc9a89"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jenniferbc/bert-base-uncased-finetuned-ner-lung/commit/c052d5a50aa6dd55d5efe832d142c7758278be5b', commit_message='End of training', commit_description='', oid='c052d5a50aa6dd55d5efe832d142c7758278be5b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/jenniferbc/bert-base-uncased-finetuned-ner-lung', endpoint='https://huggingface.co', repo_type='model', repo_id='jenniferbc/bert-base-uncased-finetuned-ner-lung'), pr_revision=None, pr_num=None)"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15f6dc-1f0b-4244-9cf9-6aa2521c43a9",
   "metadata": {
    "id": "bf15f6dc-1f0b-4244-9cf9-6aa2521c43a9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b4ea8e20-73c4-4378-f0f4-97f76193c4b5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-CANCER_CONCEPT',\n",
       " 'B-CHEMOTHERAPY',\n",
       " 'B-DATE',\n",
       " 'B-DRUG',\n",
       " 'B-FAMILY',\n",
       " 'B-FREQ',\n",
       " 'B-IMPLICIT_DATE',\n",
       " 'B-INTERVAL',\n",
       " 'B-METRIC',\n",
       " 'B-OCURRENCE_EVENT',\n",
       " 'B-QUANTITY',\n",
       " 'B-RADIOTHERAPY',\n",
       " 'B-SMOKER_STATUS',\n",
       " 'B-STAGE',\n",
       " 'B-SURGERY',\n",
       " 'B-TNM',\n",
       " 'I-CANCER_CONCEPT',\n",
       " 'I-DATE',\n",
       " 'I-DRUG',\n",
       " 'I-FAMILY',\n",
       " 'I-FREQ',\n",
       " 'I-IMPLICIT_DATE',\n",
       " 'I-INTERVAL',\n",
       " 'I-METRIC',\n",
       " 'I-OCURRENCE_EVENT',\n",
       " 'I-SMOKER_STATUS',\n",
       " 'I-STAGE',\n",
       " 'I-SURGERY',\n",
       " 'I-TNM',\n",
       " 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "label_names =  dataset_dict[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bd3ef-e0eb-47b1-8105-2281ae2700b4",
   "metadata": {
    "id": "346bd3ef-e0eb-47b1-8105-2281ae2700b4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "ec314727-e348-48ac-b092-f0b1a605cd42"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'CANCER_CONCEPT': {'precision': np.float64(0.8994910941475827),\n",
       "  'recall': np.float64(0.9241830065359478),\n",
       "  'f1': np.float64(0.9116698903932947),\n",
       "  'number': np.int64(765)},\n",
       " 'CHEMOTHERAPY': {'precision': np.float64(0.9794871794871794),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(0.9896373056994818),\n",
       "  'number': np.int64(191)},\n",
       " 'DATE': {'precision': np.float64(0.9759797724399494),\n",
       "  'recall': np.float64(0.9834394904458599),\n",
       "  'f1': np.float64(0.9796954314720813),\n",
       "  'number': np.int64(785)},\n",
       " 'DRUG': {'precision': np.float64(0.9177126917712691),\n",
       "  'recall': np.float64(0.9748148148148148),\n",
       "  'f1': np.float64(0.9454022988505747),\n",
       "  'number': np.int64(675)},\n",
       " 'FAMILY': {'precision': np.float64(0.9738562091503268),\n",
       "  'recall': np.float64(0.9933333333333333),\n",
       "  'f1': np.float64(0.9834983498349835),\n",
       "  'number': np.int64(150)},\n",
       " 'FREQ': {'precision': np.float64(0.896551724137931),\n",
       "  'recall': np.float64(0.9837837837837838),\n",
       "  'f1': np.float64(0.9381443298969072),\n",
       "  'number': np.int64(185)},\n",
       " 'IMPLICIT_DATE': {'precision': np.float64(0.40476190476190477),\n",
       "  'recall': np.float64(0.6296296296296297),\n",
       "  'f1': np.float64(0.4927536231884058),\n",
       "  'number': np.int64(27)},\n",
       " 'INTERVAL': {'precision': np.float64(0.8333333333333334),\n",
       "  'recall': np.float64(0.9523809523809523),\n",
       "  'f1': np.float64(0.888888888888889),\n",
       "  'number': np.int64(21)},\n",
       " 'METRIC': {'precision': np.float64(0.9398648648648649),\n",
       "  'recall': np.float64(0.9520876112251883),\n",
       "  'f1': np.float64(0.9459367562053723),\n",
       "  'number': np.int64(1461)},\n",
       " 'OCURRENCE_EVENT': {'precision': np.float64(0.7961476725521669),\n",
       "  'recall': np.float64(0.7811023622047244),\n",
       "  'f1': np.float64(0.7885532591414943),\n",
       "  'number': np.int64(635)},\n",
       " 'QUANTITY': {'precision': np.float64(0.9478764478764479),\n",
       "  'recall': np.float64(0.9866041527126591),\n",
       "  'f1': np.float64(0.9668526419428947),\n",
       "  'number': np.int64(1493)},\n",
       " 'RADIOTHERAPY': {'precision': np.float64(0.9157894736842105),\n",
       "  'recall': np.float64(0.9775280898876404),\n",
       "  'f1': np.float64(0.9456521739130435),\n",
       "  'number': np.int64(89)},\n",
       " 'SMOKER_STATUS': {'precision': np.float64(0.803030303030303),\n",
       "  'recall': np.float64(0.9636363636363636),\n",
       "  'f1': np.float64(0.8760330578512396),\n",
       "  'number': np.int64(55)},\n",
       " 'STAGE': {'precision': np.float64(0.9567901234567902),\n",
       "  'recall': np.float64(0.9872611464968153),\n",
       "  'f1': np.float64(0.9717868338557994),\n",
       "  'number': np.int64(157)},\n",
       " 'SURGERY': {'precision': np.float64(0.8666666666666667),\n",
       "  'recall': np.float64(0.8348623853211009),\n",
       "  'f1': np.float64(0.8504672897196262),\n",
       "  'number': np.int64(109)},\n",
       " 'TNM': {'precision': np.float64(0.9234449760765551),\n",
       "  'recall': np.float64(0.9601990049751243),\n",
       "  'f1': np.float64(0.9414634146341464),\n",
       "  'number': np.int64(201)},\n",
       " 'overall_precision': np.float64(0.9208882720333103),\n",
       " 'overall_recall': np.float64(0.9479925703671953),\n",
       " 'overall_f1': np.float64(0.9342438749647987),\n",
       " 'overall_accuracy': 0.9750849978750531}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "SUBIR MODELO A HUGGIN FACE HUB"
   ],
   "metadata": {
    "id": "22oBx7eu3qhz"
   },
   "id": "22oBx7eu3qhz"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}